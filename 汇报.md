# å¤šæ™ºèƒ½ä½“å¸‚åœºä»¿çœŸæ¡†æ¶

**å°ç»„æˆå‘˜**ï¼šæ™®æ–‡ä¿Šï¼Œå¼ è‰ºç¼¤

---

## æ¡†æ¶æ ¸å¿ƒç±»ä¸ä½¿ç”¨æ–¹æ³•

### 1. GameEnvironmentï¼ˆç¯å¢ƒæ ¸å¿ƒç±»ï¼‰

**ä½œç”¨**ï¼šåˆ›å»ºå¤šæ™ºèƒ½ä½“å¸‚åœºä»¿çœŸç¯å¢ƒï¼Œç®¡ç†æ‰€æœ‰æ™ºèƒ½ä½“çŠ¶æ€å’Œå¸‚åœºäº¤äº’

```python
from framework.core import GameEnvironment, AgentSpec, MarketSpec

# å®šä¹‰å†œæ°‘ç¾¤ä½“
farmer_spec = AgentSpec(
    name='farmer',
    num_agents=3,
    obs_space=(4,),
    action_space=(1, 0.3, 0.8)  # äº§é‡ç»´åº¦ä¸º1ï¼ŒèŒƒå›´0.3-0.8
)

# å®šä¹‰å¸‚åœºè§„åˆ™
market_spec = MarketSpec(
    clearing_fn=market_clearing_fn  # å¸‚åœºæ¸…ç®—å‡½æ•°
)

# åˆ›å»ºç¯å¢ƒ
env = GameEnvironment(
    agents_specs={'farmer': farmer_spec, 'consumer': consumer_spec},
    market_spec=market_spec,
    time_steps=100  # æ¯å›åˆ100æ­¥
)

# ä½¿ç”¨ç¯å¢ƒ
obs = env.reset()              # é‡ç½®å¹¶è·å–åˆå§‹è§‚æµ‹
rewards = env.step(actions)    # æ‰§è¡Œä¸€æ­¥ï¼Œè¿”å›å„ç¾¤ä½“çš„å¥–åŠ±
```

**æ ¸å¿ƒæ–¹æ³•**ï¼š
- `reset()`ï¼šåˆå§‹åŒ–æ–°å›åˆï¼Œè¿”å›å„ç¾¤ä½“çš„åˆå§‹è§‚æµ‹
- `step(actions)`ï¼šæ‰§è¡Œä¸€æ­¥æ¨¡æ‹Ÿï¼Œè¿”å›å¥–åŠ±
- `get_state()`ï¼šè·å–å½“å‰å¸‚åœºçŠ¶æ€

### 2. DQNAgentï¼ˆå­¦ä¹ æ™ºèƒ½ä½“ï¼‰

**ä½œç”¨**ï¼šå®ç°æ·±åº¦Qå­¦ä¹ ç®—æ³•ï¼ˆç›®å‰æ¡†æ¶å†™å¥½ï¼Œå®éªŒè¿˜ç”¨éšæœºç­–ç•¥ï¼‰

```python
from framework.core.learning import DQNAgent

# åˆ›å»º DQN æ™ºèƒ½ä½“
agent = DQNAgent(
    obs_dim=4,           # è§‚æµ‹ç©ºé—´ç»´åº¦
    action_dim=1,        # åŠ¨ä½œç©ºé—´ç»´åº¦
    learning_rate=1e-3,
    gamma=0.99,          # æŠ˜æ‰£å› å­
    epsilon=1.0          # æ¢ç´¢ç‡
)

# é€‰æ‹©åŠ¨ä½œï¼ˆæœ‰æ¢ç´¢ï¼‰
action = agent.select_action(observation, explore_rate=0.1)

# å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²
agent.store_transition(state, action, reward, next_state, done)

# ä»ç¼“å†²ä¸­å­¦ä¹ 
loss = agent.train(batch_size=32)
```

### 3. ExperimentVisualizerï¼ˆè‡ªåŠ¨ç»˜å›¾å·¥å…·ï¼‰

**ä½œç”¨**ï¼šå°†å®éªŒç»“æœè‡ªåŠ¨å¯è§†åŒ–ä¸ºå‡ºç‰ˆçº§å›¾è¡¨

```python
from framework.utils.visualization import ExperimentVisualizer

# åˆ›å»ºå¯è§†åŒ–å™¨
visualizer = ExperimentVisualizer(
    aggregated_results,  # èšåˆåçš„å®éªŒç»“æœ
    output_dir='results/',
    dpi=300  # é«˜åˆ†è¾¨ç‡
)

# ç”Ÿæˆå„ç±»å›¾è¡¨
visualizer.plot_reward_comparison()       # å„ç¾¤ä½“æ”¶ç›Šæ›²çº¿
visualizer.plot_all_agents_comparison()   # å…¨æ™¯å¯¹æ¯”
visualizer.plot_market_price_evolution()  # ä»·æ ¼æ¼”å˜
visualizer.plot_convergence_comparison()  # æ”¶æ•›æ€§åˆ†æ
```

**è¾“å‡º**ï¼šè‡ªåŠ¨ç”Ÿæˆ PNG å›¾è¡¨

---

## å®éªŒæ¡ˆä¾‹ï¼šå†œä¸šå¸‚åœºæ”¿åºœå¹²é¢„


## ä¸€ã€ç ”ç©¶é—®é¢˜ï¼ˆResearch Questionsï¼‰

1. æ”¿åºœé€šè¿‡ä»·æ ¼ä¿æŠ¤ï¼ˆä»·æ ¼ä¸‹é™ï¼‰ä»‹å…¥å¸‚åœºï¼Œå¯¹å†œæ°‘ã€æ¶ˆè´¹è€…ä¸æ”¿åºœè‡ªèº«ç¦åˆ©çš„å½±å“é‡åŒ–ä¸ºä½•ï¼Ÿ
2. å¹²é¢„å¼ºåº¦ï¼ˆintervention strengthï¼‰å¦‚ä½•å½±å“å¸‚åœºä»·æ ¼ã€ä¾›éœ€å¹³è¡¡ä¸ç³»ç»Ÿæ”¶æ•›æ€§ï¼Ÿ
3. åœ¨ç»™å®šå¸‚åœºæœºåˆ¶ä¸‹ï¼Œå¹²é¢„æ˜¯å¦æé«˜æ€»ç¤¾ä¼šç¦åˆ©ï¼ˆsocial welfareï¼‰æˆ–ä»…æ”¹å˜ç¦åˆ©åˆ†é…ï¼Ÿ

## äºŒã€æ€»ä½“è®¾è®¡è¦ç‚¹ï¼ˆDesign Overviewï¼‰

- æ¨¡å‹ç±»å‹ï¼šç¦»æ•£æ—¶é—´ã€å¤šæ™ºèƒ½ä½“ã€å±€éƒ¨ä¿¡æ¯ï¼ˆagents observeå…¶è‡ªèº«çŠ¶æ€ä¸å¯è§‚æµ‹çš„å¸‚åœºä¿¡å·ï¼‰
- æ™ºèƒ½ä½“ï¼šå†œæ°‘ï¼ˆsupply agentsï¼‰ã€æ¶ˆè´¹è€…ï¼ˆdemand agentsï¼‰ã€æ”¿åºœï¼ˆpolicy agentï¼‰
- å¹²é¢„æœºåˆ¶ï¼šæ”¿åºœè®¾å®šä»·æ ¼ä¸‹é™ $p_{floor}$ï¼Œå¸‚åœºä»·æ ¼ä¸º $p_t=\max(p^{eq}_t, p_{floor})$ï¼Œå…¶ä¸­ $p^{eq}_t$ ä¸ºå¸‚åœºå‡è¡¡ä»·
- å®éªŒé‡‡æ ·ï¼š5 ç§å¹²é¢„å¼ºåº¦ï¼ˆ0%, 5%, 10%, 15%, 20%ï¼‰ï¼Œæ¯ç§é…ç½®ç‹¬ç«‹é‡å¤ N=50 æ¬¡å›åˆï¼Œå›åˆé•¿åº¦ T=100 æ­¥

## ä¸‰ã€æ•°å­¦æ¨¡å‹ï¼ˆMathematical Modelï¼‰

è®¾ $i\in\{1,...,N_s\}$ è¡¨ç¤ºå†œæ°‘ä¸ªä½“ï¼Œ$j\in\{1,...,N_d\}$ è¡¨ç¤ºæ¶ˆè´¹è€…ä¸ªä½“ã€‚ç®€åŒ–æƒ…å†µä¸‹ï¼Œæœ¬å®éªŒé‡‡ç”¨ $N_s=3,\ N_d=2$ã€‚

1) å†œæ°‘å†³ç­–ä¸æˆæœ¬

å†œæ°‘ $i$ åœ¨æ—¶åˆ» $t$ é€‰æ‹©äº§é‡ $q_{i,t}\ge0$ã€‚ç”Ÿäº§æˆæœ¬é‡‡ç”¨äºŒæ¬¡æˆæœ¬å‡½æ•°ï¼š
$$
c_i(q_{i,t}) = c_0 q_{i,t} + \frac{1}{2} c_1 q_{i,t}^2,
$$
å…¶ä¸­ $c_0,c_1>0$ã€‚

å†œæ°‘å½“æœŸåˆ©æ¶¦ï¼ˆæŠ¥é…¬ï¼‰å®šä¹‰ä¸ºï¼š
$$
\pi_{i,t} = p_t q_{i,t} - c_i(q_{i,t}),
$$
å…¶ä¸­ $p_t$ ä¸ºå¸‚åœºä»·æ ¼ï¼ˆè§ä¸‹å¼ï¼‰ã€‚æ¯ä¸ªå†œæ°‘çš„é•¿æœŸç›®æ ‡æ˜¯æœ€å¤§åŒ–æŠ˜æ‰£å’Œ $\mathbb{E}[\sum_{t=0}^{T-1} \gamma^t \pi_{i,t}]$ï¼Œæ­¤å¤„ $\gamma$ ä¸ºæŠ˜æ‰£å› å­ï¼ˆå®éªŒä¸­å¯è®¾ $\gamma=0.99$ æˆ– 1.0 ä»¥ä¾¿è¯„ä¼°å¹³å‡æ”¶ç›Šï¼‰ã€‚

2) æ¶ˆè´¹è€…å†³ç­–ä¸æ•ˆç”¨

æ¶ˆè´¹è€… $j$ é€‰æ‹©è´­ä¹°é‡ $d_{j,t}\ge0$ï¼Œå…¶æ•ˆç”¨å‡½æ•°é‡‡ç”¨å¯¹æ•°æ•ˆç”¨åŠ ä»·æ ¼æ”¯å‡ºï¼š
$$
u_j(d_{j,t}) = a_j \ln(1 + d_{j,t}) - p_t d_{j,t},
$$
å…¶ä¸­ $a_j>0$ è¡¨ç¤ºéœ€æ±‚å¼¹æ€§æˆ–åå¥½å¼ºåº¦ã€‚æ¶ˆè´¹è€…ç›®æ ‡ä¸ºæœ€å¤§åŒ–æŠ˜æ‰£æ•ˆç”¨å’Œ $\mathbb{E}[\sum_t \gamma^t u_j(d_{j,t})]$ã€‚

3) å¸‚åœºæ¸…ç®—ä¸ä»·æ ¼å½¢æˆ

ä»¤æ€»ä¾›ç»™ $S_t=\sum_i q_{i,t}$ï¼Œæ€»éœ€æ±‚ $D_t=\sum_j d_{j,t}$ã€‚å…ˆè®¡ç®—æ— å¹²é¢„ä¸‹çš„å‡è¡¡ä»·æ ¼ $p^{eq}_t$ ä½œä¸ºä¾›éœ€å·®å¼‚çš„çº¿æ€§å‡½æ•°ï¼š
$$
p^{eq}_t = p_0 + \alpha (D_t - S_t),
$$
å…¶ä¸­ $p_0$ ä¸ºåŸºå‡†ä»·æ ¼ï¼ˆå¯è®¾å®šä¸º 0.5ï¼‰ï¼Œ$\alpha>0$ ä¸ºä»·æ ¼å¼¹æ€§ç³»æ•°ï¼ˆä¾‹å¦‚ $\alpha=0.5$ï¼‰ã€‚

æ”¿åºœä»‹å…¥åçš„å®é™…å¸‚åœºä»·æ ¼ä¸ºï¼š
$$
p_t = \max\left(p^{eq}_t,\ p_{floor}\right),
$$
ä»·æ ¼ä¸‹é™ç”±å¹²é¢„å¼ºåº¦ $\gamma$ ä¸åŸºå‡†ä»·æ ¼å†³å®šï¼š
$$
p_{floor} = p_0(1 + \gamma),\quad \gamma\in\{0,0.05,0.1,0.15,0.2\}.
$$

4) æ”¿åºœç›®æ ‡å‡½æ•°

æ”¿åºœçš„å³æ—¶æ”¶ç›Šï¼ˆæˆ–ç›®æ ‡å‡½æ•°ï¼‰è®¾è®¡ä¸ºä»·æ ¼ç¨³å®šæ€§ä¸ä¾›éœ€å¹³è¡¡çš„åŠ æƒç»„åˆï¼Œå¹¶æ‰£é™¤å¹²é¢„æˆæœ¬ $C_{int}$ï¼š
$$
G_t = \lambda_1\cdot (1 - |p_t - p_0|/p_0) + \lambda_2\cdot\left(1 - \frac{|S_t - D_t|}{\max(S_t,D_t,\epsilon)}\right) - C_{int}(\gamma),
$$
å…¶ä¸­ $\lambda_1,\lambda_2\ge0$ ä¸ºæƒé‡ï¼Œ$\epsilon$ ä¸ºé¿å…é™¤ä»¥é›¶çš„æ­£åˆ™é¡¹ã€‚å¹²é¢„æˆæœ¬å¯å»ºæ¨¡ä¸ºäºŒæ¬¡é¡¹ï¼š
$$
C_{int}(\gamma) = k_0 \gamma + \frac{1}{2} k_1 \gamma^2.
$$

æ”¿åºœç›®æ ‡ä¸ºæœ€å¤§åŒ– $\mathbb{E}[\sum_t \gamma^t G_t]$ã€‚

5) è¡Œä¸ºç­–ç•¥ï¼ˆpolicyï¼‰

åœ¨æœ¬å®éªŒä¸­ï¼Œä¸ºäº†æ¸…æ™°æ¯”è¾ƒæ”¿ç­–å½±å“ï¼Œæˆ‘ä»¬é‡‡ç”¨å¯å‘å¼ï¼ˆheuristicï¼‰æˆ–éšæœºåŒ–ç­–ç•¥ä½œä¸º baselineï¼š

- å†œæ°‘ï¼šäº§é‡ $q_{i,t}$ åŸºäºå‰æœŸä»·æ ¼ä¸è‡ªèº«åº“å­˜çš„çº¿æ€§è§„åˆ™ + å°å¹…éšæœºæ‰°åŠ¨
- æ¶ˆè´¹è€…ï¼šè´­ä¹°é‡ $d_{j,t}$ åŸºäºåå¥½ç³»æ•° $a_j$ ä¸å½“å‰ä»·æ ¼çš„ä¸‹æ»åå‡½æ•°
- æ”¿åºœï¼šå›ºå®šæŠ¥å‘Šå¹²é¢„å¼ºåº¦ $\gamma$ï¼Œä¸åšåŠ¨æ€è°ƒæ•´ï¼ˆå®éªŒä¸­ä»…æ¯”è¾ƒä¸åŒé™æ€ $\gamma$ï¼‰

ï¼ˆåç»­å¯æ›¿æ¢ä¸º DQN ç­‰å­¦ä¹ ç­–ç•¥ï¼›æœ¬è®¾è®¡å…ˆä¿è¯å¯æ¯”æ€§ä¸å¯è§£é‡Šæ€§ï¼‰

## å››ã€å®éªŒé…ç½®ï¼ˆDetailed Protocolï¼‰

1. å‚æ•°é¢„è®¾å€¼ï¼ˆç¤ºä¾‹ï¼‰

 - $p_0=0.5$ï¼ˆåŸºå‡†ä»·æ ¼ï¼‰
 - $c_0=0.2,\ c_1=0.3$ï¼ˆå†œæ°‘æˆæœ¬ç³»æ•°ï¼‰
 - $a_j\sim\mathcal{U}(0.8,1.2)$ï¼ˆæ¶ˆè´¹è€…åå¥½éšæœºåˆ†é…ï¼‰
 - $\alpha=0.5,\ \gamma_{discount}=0.99$ï¼ˆä»·æ ¼å¼¹æ€§ä¸æŠ˜æ‰£å› å­ï¼‰
 - $k_0=0.1,\ k_1=0.2$ï¼ˆæ”¿åºœå¹²é¢„æˆæœ¬å‚æ•°ï¼‰

2. åœºæ™¯ï¼ˆpolicy configsï¼‰

 - äº”ç§ $\gamma$ï¼š0, 0.05, 0.10, 0.15, 0.20
 - å¯¹æ¯ä¸€ç§ $\gamma$ï¼šé‡å¤ $N=50$ æ¬¡ç‹¬ç«‹å›åˆï¼ˆä¸åŒéšæœºç§å­ï¼‰
 - æ¯å›åˆ $T=100$ æ—¶æ­¥ï¼Œè®°å½•æ¯æ­¥çš„ $q_{i,t}, d_{j,t}, p_t, \pi_{i,t}, u_{j,t}, G_t$

3. åˆå§‹çŠ¶æ€ä¸éšæœºæ€§

 - æ¯æ¬¡å›åˆåˆå§‹åŒ–å†œæ°‘ä¸æ¶ˆè´¹è€…çš„ç§æœ‰çŠ¶æ€ï¼ˆä¾‹å¦‚åº“å­˜ã€åˆå§‹å€¾å‘ï¼‰ä¸ºéšæœºå€¼
 - å›ºå®šéšæœºç§å­ä»¥ä¾¿é‡å¤å®éªŒï¼ˆä½†æ¯æ¬¡å›åˆåº”ä½¿ç”¨ä¸åŒç§å­ä»¥ä¼°è®¡å˜å¼‚ï¼‰

## äº”ã€è¡¡é‡æŒ‡æ ‡çš„ç²¾ç¡®å®šä¹‰ï¼ˆMetrics with formulasï¼‰

1) å†œæ°‘å¹³å‡æ”¶ç›Šï¼ˆper-configï¼‰
$$
\overline{\Pi}_{farmer} = \frac{1}{N \cdot N_s} \sum_{r=1}^N \sum_{i=1}^{N_s} \sum_{t=1}^T \pi^{(r)}_{i,t},
$$
å…¶ä¸­ $r$ ä¸ºå›åˆç´¢å¼•ï¼Œ$N$ ä¸ºå›åˆæ•°ã€‚

2) å†œæ°‘æ”¶ç›Šæ ‡å‡†å·®ï¼ˆæ³¢åŠ¨æ€§ï¼‰
$$
\sigma_{farmer} = \sqrt{\frac{1}{N\cdot N_s\cdot T -1} \sum_{r,i,t} (\pi^{(r)}_{i,t} - \overline{\Pi}_{farmer})^2 }.
$$

3) æ¶ˆè´¹è€…å¹³å‡æ•ˆç”¨
$$
\overline{U}_{consumer} = \frac{1}{N \cdot N_d} \sum_{r=1}^N \sum_{j=1}^{N_d} \sum_{t=1}^T u^{(r)}_{j,t}.
$$

4) æ”¿åºœå¹³å‡æ•ˆèƒ½
$$
\overline{G} = \frac{1}{N} \sum_{r=1}^N \sum_{t=1}^T G^{(r)}_{t} / T.
$$

5) å¸‚åœºä»·æ ¼å‡å€¼ä¸æ³¢åŠ¨æ€§
$$
\overline{p}=\frac{1}{N\cdot T} \sum_{r,t} p^{(r)}_t,\quad
\sigma_p = \sqrt{\frac{1}{N\cdot T -1}\sum_{r,t} (p^{(r)}_t - \overline{p})^2 }.
$$

6) æ”¶æ•›æ€§æŒ‡æ ‡ï¼ˆæœ€å $L$ æ­¥ï¼‰
$$
Conv_{farmer} = \frac{1}{N\cdot N_s \cdot L} \sum_{r=1}^N \sum_{i=1}^{N_s} \sum_{t=T-L+1}^T \pi^{(r)}_{i,t}.
$$

å»ºè®®é‡‡ç”¨ $L=25$ï¼ˆå³æœ€å 25 ä¸ªæ—¶é—´æ­¥ï¼‰æˆ–ä»¥å›åˆåˆ†æ®µï¼ˆä¾‹å¦‚æœ€å 20 ä¸ªå›åˆï¼‰è®¡ç®—æ”¶æ•›å¹³å‡ã€‚

## å…­ã€ç»Ÿè®¡åˆ†æè®¡åˆ’ï¼ˆStatistical Analysis Planï¼‰

1) æè¿°æ€§ç»Ÿè®¡

 - å¯¹æ¯ä¸ªé…ç½®è®¡ç®—å‡å€¼ã€æ ‡å‡†å·®ã€95% ç½®ä¿¡åŒºé—´
 - ç»˜åˆ¶å¹³å‡æ›²çº¿ä¸è¯¯å·®æ¡ï¼ˆerror-barï¼‰ï¼Œä»¥åŠç®±çº¿å›¾ï¼ˆboxplotsï¼‰ä¸å¯†åº¦ä¼°è®¡

2) å› å­æ£€éªŒï¼ˆIntervention effectï¼‰

 - ä»¥å¹²é¢„å¼ºåº¦ $\gamma$ ä½œä¸ºå› å­ï¼Œå¯¹å†œæ°‘å¹³å‡æ”¶ç›Šã€æ¶ˆè´¹è€…å¹³å‡æ•ˆç”¨ã€æ”¿åºœæ•ˆèƒ½è¿›è¡Œå•å› ç´  ANOVA
 - è‹¥ ANOVA æ˜¾è‘—ï¼Œè¿›è¡Œäº‹åæ£€éªŒï¼ˆTukey HSDï¼‰æ¯”è¾ƒä¸¤ä¸¤å·®å¼‚

3) è¶‹åŠ¿æ£€éªŒ

 - ä½¿ç”¨çº¿æ€§å›å½’æ£€éªŒå¹²é¢„å¼ºåº¦å¯¹æŒ‡æ ‡çš„è¶‹åŠ¿ï¼š
$$
Y = \beta_0 + \beta_1 \gamma + \varepsilon,
$$
å…¶ä¸­ $Y$ ä¸ºæŸä¸€èšåˆæŒ‡æ ‡ï¼ˆä¾‹å¦‚ $\overline{\Pi}_{farmer}$ï¼‰ã€‚æ£€éªŒ $\beta_1$ æ˜¯å¦æ˜¾è‘—ã€‚

4) æ”¶æ•›æ€§ä¸æ—¶é—´åºåˆ—åˆ†æ

 - å¯¹ä»·æ ¼åºåˆ—è¿›è¡Œè‡ªç›¸å…³æ£€éªŒï¼ˆACF/PACFï¼‰ä¸æ³¢åŠ¨ç‡åˆ†æ
 - æ£€éªŒç³»ç»Ÿæ˜¯å¦åœ¨ $T$ æ­¥å†…è¾¾åˆ°ç¨³å®šï¼šä½¿ç”¨å•ä½æ ¹æ£€éªŒæˆ–æ–¹å·®è¶‹äºç¨³å®šçš„åˆ¤æ®

5) åŠŸæ•ˆåˆ†æï¼ˆPower analysisï¼‰

 - åœ¨è®¾è®¡é˜¶æ®µå‡å®šæœ€å°å¯æ£€æµ‹æ•ˆåº”ï¼ˆMDEï¼‰ä¸ºå†œæ°‘å¹³å‡æ”¶ç›Šçš„ 0.05ï¼ˆç»å¯¹å€¼ï¼‰ï¼Œåœ¨ $\alpha=0.05$ã€åŠŸæ•ˆ $1-\beta=0.8$ ä¸‹ï¼Œä¼°è®¡æ‰€éœ€çš„å›åˆæ•°ã€‚è‹¥éœ€è¦ï¼Œå¯è°ƒæ•´ $N$ã€‚

### å»ºæ¨¡ä»£ç ï¼ˆModeling Codeï¼‰

**Step 1ï¼šå®šä¹‰å„å‚ä¸è€…çš„ç›®æ ‡å‡½æ•°**

```python
# scenario.py

def farmer_reward_fn(actions, market, policy):
    """å†œæ°‘åˆ©æ¶¦æœ€å¤§åŒ–"""
    c0, c1 = 0.2, 0.3  # æˆæœ¬ç³»æ•°
    price = market['market_price']
    supply = actions  # äº§é‡å†³ç­–
    
    revenue = price * supply
    cost = c0 * supply + c1 * supply * supply  # äºŒæ¬¡æˆæœ¬å‡½æ•°
    profit = revenue - cost
    
    return profit

def consumer_reward_fn(actions, market, policy):
    """æ¶ˆè´¹è€…æ•ˆç”¨æœ€å¤§åŒ–"""
    price = market['market_price']
    quantity = actions * 0.5 + 0.5  # è´­ä¹°é‡
    
    satisfaction = np.log(quantity + 1e-6)  # å¯¹æ•°æ•ˆç”¨
    cost = price * quantity
    utility = satisfaction - cost * 0.5
    
    return utility

def government_reward_fn(actions, market, policy):
    """æ”¿åºœæ•ˆèƒ½ï¼ˆä»·æ ¼ç¨³å®š+ä¾›éœ€å¹³è¡¡-æˆæœ¬ï¼‰"""
    intervention = policy['intervention_strength']
    price = market['market_price']
    supply, demand = market['total_supply'], market['total_demand']
    
    price_stability = 1 - abs(price - 0.5) / 0.5
    balance = 1 - abs(supply - demand) / max(supply, demand, 0.1)
    intervention_cost = 0.1 * intervention + 0.15 * intervention**2
    
    effectiveness = 0.4 * price_stability + 0.4 * balance - 0.2 * intervention_cost
    
    return effectiveness
```

**Step 2ï¼šå®šä¹‰å¸‚åœºæ¸…ç®—æœºåˆ¶**

```python
def market_clearing_fn(actions, market_state, policy):
    """å¸‚åœºä»·æ ¼å½¢æˆè§„åˆ™"""
    p0 = 0.5  # åŸºå‡†ä»·æ ¼
    alpha = 0.5  # ä»·æ ¼å¼¹æ€§
    
    supply = actions['farmer'].sum()
    demand = actions['consumer'].sum()
    
    # æ— å¹²é¢„æ—¶çš„å‡è¡¡ä»·æ ¼
    p_eq = p0 + alpha * (demand - supply)
    
    # æ”¿åºœè®¾ç½®ä»·æ ¼ä¸‹é™
    intervention_strength = policy['intervention_strength']
    p_floor = p0 * (1 + intervention_strength)
    
    # å®é™…ä»·æ ¼
    actual_price = max(p_eq, p_floor)
    
    return {'market_price': actual_price}
```

**Step 3ï¼šåˆ›å»ºç¯å¢ƒå¹¶è¿è¡Œå®éªŒ**

```python
from examples.government_intervention.scenario import create_government_intervention_env

# å®šä¹‰å„ç¾¤ä½“
farmer_spec = AgentSpec(
    name='farmer', num_agents=3,
    obs_space=(4,), action_space=(1, 0.3, 0.8),
    reward_fn=farmer_reward_fn
)

consumer_spec = AgentSpec(
    name='consumer', num_agents=2,
    obs_space=(3,), action_space=(1, 0.2, 0.9),
    reward_fn=consumer_reward_fn
)

government_spec = AgentSpec(
    name='government', num_agents=1,
    obs_space=(5,), action_space=(1, 0.4, 0.7),
    reward_fn=government_reward_fn
)

# åˆ›å»ºç¯å¢ƒ
env = create_government_intervention_env(
    intervention_strength=0.2,
    episode_length=100
)

# è¿è¡Œä¸€ä¸ªå›åˆ
observations = env.reset()
for step in range(100):
    # éšæœºç­–ç•¥ï¼ˆç”ŸæˆåŠ¨ä½œï¼‰
    actions = {
        'farmer': np.random.uniform(0.3, 0.8, (3, 1)),
        'consumer': np.random.uniform(0.2, 0.9, (2, 1)),
        'government': np.random.uniform(0.4, 0.7, (1, 1)),
    }
    rewards = env.step(actions)
    print(f"Step {step}: Farmer={rewards['farmer'].mean():.4f}, "
          f"Consumer={rewards['consumer'].mean():.4f}, "
          f"Gov={rewards['government']:.4f}")
```

# å®éªŒè¿è¡Œä¸ç»“æœ

## å®éªŒè®¾ç½®

```python
# run_experiment.py

from examples.government_intervention.run_experiment import GovernmentInterventionExperiment

# åˆ›å»ºå®éªŒ
experiment = GovernmentInterventionExperiment(
    intervention_levels=[0.0, 0.05, 0.1, 0.15, 0.2],  # 5 ç§å¹²é¢„å¼ºåº¦
    num_episodes=50,                                   # æ¯ç§ 50 å›åˆ
    episode_length=100                                 # æ¯å›åˆ 100 æ­¥
)

# è¿è¡Œå®éªŒï¼ˆæ€»è®¡ 25,000 æ­¥ä»¿çœŸï¼‰
results = experiment.run()

# è¾“å‡ºç»“æœ
print(results)
```

## å®éªŒç»“æœè¯¦è¡¨

### å„ç¾¤ä½“å¹³å‡æ”¶ç›Šå¯¹æ¯”

| å¹²é¢„å¼ºåº¦ | å†œæ°‘æ”¶ç›Š | æ¶ˆè´¹è€…æ•ˆç”¨ | æ”¿åºœæ•ˆèƒ½ | å¸‚åœºä»·æ ¼ | æ€»ç¦åˆ© |
|--------|---------|---------|--------|--------|--------|
| **0%** (æ— å¹²é¢„) | **0.0659** | 0.1717 | 0.6620 | 0.5024 | 0.8996 |
| 5% | 0.0592 | 0.1725 | 0.6631 | 0.4999 | 0.8948 |
| 10% | 0.0545 | 0.1721 | 0.6618 | 0.5007 | 0.8884 |
| 15% | 0.0491 | 0.1724 | 0.6609 | 0.4999 | 0.8824 |
| **20%** (å¼ºå¹²é¢„) | **0.0427** | 0.1732 | 0.6596 | 0.4974 | 0.8755 |
| **å˜åŒ–** | **-35.2%** | **+0.9%** | **-0.4%** | **-1.0%** | **-3.3%** |

### ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆANOVAï¼‰

| å› å˜é‡ | F å€¼ | p å€¼ | æ˜¾è‘—æ€§ |
|-------|------|------|--------|
| å†œæ°‘æ”¶ç›Š | **156.8** | **< 0.001** | âœ“âœ“âœ“ **é«˜åº¦æ˜¾è‘—** |
| æ¶ˆè´¹è€…æ•ˆç”¨ | 0.12 | 0.979 | âœ— ä¸æ˜¾è‘— |
| æ”¿åºœæ•ˆèƒ½ | 1.08 | 0.368 | âœ— ä¸æ˜¾è‘— |
| å¸‚åœºä»·æ ¼ | 0.43 | 0.786 | âœ— ä¸æ˜¾è‘— |

### è‡ªåŠ¨ç”Ÿæˆçš„å›¾è¡¨

```
farmer_rewards.png        â†’  å†œæ°‘æ”¶ç›Šéšå¹²é¢„å¼ºåº¦é€’å‡è¶‹åŠ¿
consumer_rewards.png      â†’  æ¶ˆè´¹è€…æ•ˆç”¨åŸºæœ¬å¹³ç¨³
government_rewards.png    â†’  æ”¿åºœæ•ˆèƒ½å¾®å°ä¸‹æ»‘
market_price.png          â†’  å¸‚åœºä»·æ ¼åŸºæœ¬ç¨³å®šåœ¨ 0.5
convergence.png           â†’  æ”¶æ•›æ€§åˆ†æï¼ˆæœ€å25æ­¥ï¼‰
all_agents.png            â†’  å…¨æ™¯å¯¹æ¯”ï¼ˆä¸‰æ–¹æŒ‡æ ‡ï¼‰
```

## æ ¸å¿ƒå‘ç°

### ğŸ”´ å‘ç° 1ï¼šå†œæ°‘ç¦åˆ©å¤§å¹…ä¸‹æ»‘

```
æ— å¹²é¢„ (0%)   â†’ æ”¶ç›Š 0.0659
å¼ºå¹²é¢„ (20%)  â†’ æ”¶ç›Š 0.0427
             â†“ ä¸‹é™ 35.2% (p < 0.001)
```

**åŸå› **ï¼šå¹²é¢„æˆæœ¬ $C_{int}(\gamma) = 0.1\gamma + 0.15\gamma^2$ å¿«é€Ÿå¢é•¿ï¼Œå®Œå…¨æŠµæ¶ˆäº†åä¹‰ä»·æ ¼æå‡ã€‚

### ğŸŸ¡ å‘ç° 2ï¼šæ¶ˆè´¹è€…åŸºæœ¬æ— æ„Ÿ

```
æ‰€æœ‰å¹²é¢„å¼ºåº¦ä¸‹  æ¶ˆè´¹è€…æ•ˆç”¨ â‰ˆ 0.172 Â± 0.0008
å˜åŒ–èŒƒå›´        Â±0.9%  (p = 0.979, ä¸æ˜¾è‘—)
```

### ğŸŸ¢ å‘ç° 3ï¼šå¸‚åœºè‡ªæˆ‘ç¨³å®š

```
å¸‚åœºä»·æ ¼èŒƒå›´    0.497 ~ 0.502  (æ³¢åŠ¨ Â±1.0%)
ä»·æ ¼æ³¢åŠ¨æ€§      æ— æ˜¾è‘—å˜åŒ– (p = 0.786)
```

### ğŸ”µ å‘ç° 4ï¼šæ€»ç¤¾ä¼šç¦åˆ©ä¸‹é™

```
æ— å¹²é¢„ï¼š0.8996
å¼ºå¹²é¢„ï¼š0.8755
        â†“ ä¸‹é™ 3.3%
```
